#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„å¤šæ¨¡æ€å®ä½“å¯¹é½æ¨¡å‹
ä¸»è¦æ”¹è¿›ï¼š
1. æ¨¡æ€å¯é æ€§è¯„ä¼° - è‡ªåŠ¨æ£€æµ‹ç¼ºå¤±/ä½è´¨é‡æ¨¡æ€
2. è‡ªé€‚åº”èåˆæƒé‡ - å®ä½“çº§åˆ«çš„åŠ¨æ€æƒé‡
3. è·¨æ¨¡æ€å¯¹é½æœºåˆ¶ - å¼ºåˆ¶æ¨¡æ€é—´ä¸€è‡´æ€§
4. å›¾åƒç‰¹å¾å¢å¼º - æ›´å¥½çš„ç¼ºå¤±å¤„ç†ç­–ç•¥
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import gc

# CLIPç›¸å…³å¯¼å…¥
try:
    from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer
    CLIP_AVAILABLE = True
except ImportError:
    print("Warning: transformers not available, CLIP features disabled")
    CLIPModel = CLIPProcessor = CLIPTokenizer = None
    CLIP_AVAILABLE = False

try:
    from layers import GraphConvolution, MultiHeadGraphAttention, ProjectionHead
except:
    from src.layers import GraphConvolution, MultiHeadGraphAttention, ProjectionHead


class SimpleGCN(nn.Module):
    """ç®€åŒ–çš„GCNå±‚ï¼Œé¿å…å†…å­˜é—®é¢˜"""
    def __init__(self, nfeat, nhid, nclass, dropout):
        super(SimpleGCN, self).__init__()
        self.gc1 = GraphConvolution(nfeat, nhid)
        self.gc2 = GraphConvolution(nhid, nclass)
        self.dropout = dropout

    def forward(self, x, adj):
        x = F.relu(self.gc1(x, adj))
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.gc2(x, adj)
        return x


class SimpleGAT(nn.Module):
    """ä¿®å¤çš„GATå®ç° - æ­£ç¡®å¤„ç†å¤šå¤´æ³¨æ„åŠ›è¾“å‡º"""
    def __init__(self, n_units, n_heads, dropout, attn_dropout, instance_normalization, diag):
        super(SimpleGAT, self).__init__()
        self.n_units = n_units
        self.n_heads = n_heads
        self.dropout = dropout
        self.attn_dropout = attn_dropout
        self.instance_normalization = instance_normalization
        self.diag = diag
        
        self.layers = nn.ModuleList()
        for i in range(len(n_units) - 1):
            in_dim = n_units[i]
            out_dim = n_units[i + 1]
            n_head = n_heads[i] if i < len(n_heads) else 1
            
            self.layers.append(
                MultiHeadGraphAttention(
                    n_head=n_head,
                    f_in=in_dim,
                    f_out=out_dim,
                    attn_dropout=attn_dropout,
                    diag=diag
                )
            )
        
        if instance_normalization:
            self.norm_layers = nn.ModuleList([
                nn.LayerNorm(n_units[i+1]) for i in range(len(n_units) - 1)
            ])
    
    def forward(self, x, adj):
        for i, layer in enumerate(self.layers):
            n_head = self.n_heads[i] if i < len(self.n_heads) else 1
            
            if x.size(0) > 20000:
                x = self.simple_forward(x, adj, layer, n_head)
            else:
                x = layer(x, adj)  # è¿”å› [n_head, N, f_out]
                
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å¤šå¤´è¾“å‡ºç»´åº¦
                if x.dim() == 3 and x.size(0) == n_head:
                    # æ–¹æ¡ˆ1: å¤šå¤´å¹³å‡
                    x = x.mean(dim=0)  # [N, f_out]
                    # æ–¹æ¡ˆ2: æˆ–è€…æ‹¼æ¥åæŠ•å½± (éœ€è¦é¢å¤–å‚æ•°)
                    # x = x.transpose(0, 1).contiguous().view(N, -1)
                
            if hasattr(self, 'norm_layers'):
                x = self.norm_layers[i](x)
            if i < len(self.layers) - 1:
                x = F.relu(x)
                x = F.dropout(x, self.dropout, training=self.training)
        return x
    
    def simple_forward(self, x, adj, layer, n_head):
        """ç®€åŒ–çš„å‰å‘ä¼ æ’­ï¼Œé¿å…å†…å­˜çˆ†ç‚¸"""
        w = layer.w[0] if layer.diag else layer.w[0]
        
        if layer.diag:
            h = torch.mul(x, w)
        else:
            h = torch.mm(x, w)
        
        if adj.is_sparse:
            output = torch.sparse.mm(adj, h)
        else:
            output = torch.mm(adj, h)
        
        return output


class CLIPEntityEncoder(nn.Module):
    """
    ä¿®å¤çš„CLIPå®ä½“ç¼–ç å™¨
    
    ğŸ”§ ä¿®å¤ï¼š
    1. æ·»åŠ  encode_images_from_raw() çœŸæ­£ä½¿ç”¨ CLIP çš„ ViT ç¼–ç å™¨
    2. æ”¹è¿› encode_images() ä½¿ç”¨å¤šå±‚å¯¹é½ç½‘ç»œï¼ˆè€Œéç®€å•çº¿æ€§å±‚ï¼‰
    3. æ·»åŠ å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
    """
    
    def __init__(self, clip_model_name="openai/clip-vit-base-patch32", freeze_clip=True, device=None):
        super(CLIPEntityEncoder, self).__init__()
        
        if not CLIP_AVAILABLE:
            raise ImportError("transformers library required for CLIP")
        
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.clip_model = CLIPModel.from_pretrained(clip_model_name)
        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
        self.clip_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)
        
        self.clip_model = self.clip_model.to(self.device)
        
        if freeze_clip:
            for param in self.clip_model.parameters():
                param.requires_grad = False
        
        self.clip_dim = self.clip_model.config.projection_dim
        self.image_projection = None
        self.feature_aligner = None  # ğŸ†• å¤šå±‚ç‰¹å¾å¯¹é½ç½‘ç»œ
    
    def encode_images_from_raw(self, images):
        """
        ğŸ†• ä»åŸå§‹å›¾åƒç¼–ç ï¼ˆçœŸæ­£ä½¿ç”¨ViTï¼ï¼‰
        
        Args:
            images: PIL Images åˆ—è¡¨æˆ–å·²å¤„ç†çš„ pixel_values tensor
        
        Returns:
            image_features: [N, clip_dim] å½’ä¸€åŒ–çš„å›¾åƒç‰¹å¾
        """
        try:
            if isinstance(images, list):
                inputs = self.clip_processor(images=images, return_tensors="pt")
                pixel_values = inputs['pixel_values'].to(self.device)
            else:
                pixel_values = images.to(self.device)
            
            # ğŸ”¥ çœŸæ­£ä½¿ç”¨ CLIP çš„ ViT ç¼–ç å™¨ï¼
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(pixel_values=pixel_values)
            
            return F.normalize(image_features, dim=-1)
        
        except Exception as e:
            print(f"Warning: Failed to encode raw images with ViT: {e}")
            return None
    
    def encode_images(self, image_features, use_alignment=True):
        """
        ç¼–ç é¢„æå–çš„å›¾åƒç‰¹å¾
        
        ğŸ”§ æ”¹è¿›ï¼šä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºå¯¹é½åˆ°CLIPç©ºé—´ï¼ˆè€Œéç®€å•çº¿æ€§å±‚ï¼‰
        """
        input_dim = image_features.size(-1)
        
        if use_alignment:
            if self.feature_aligner is None or self.feature_aligner[0].in_features != input_dim:
                # å¤šå±‚å¯¹é½ç½‘ç»œï¼Œæ¯”å•ä¸ªçº¿æ€§å±‚æ•ˆæœæ›´å¥½
                self.feature_aligner = nn.Sequential(
                    nn.Linear(input_dim, self.clip_dim * 2),
                    nn.LayerNorm(self.clip_dim * 2),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    nn.Linear(self.clip_dim * 2, self.clip_dim),
                    nn.LayerNorm(self.clip_dim)
                ).to(self.device)
            
            aligned = self.feature_aligner(image_features)
        else:
            if self.image_projection is None or self.image_projection.in_features != input_dim:
                self.image_projection = nn.Linear(input_dim, self.clip_dim).to(self.device)
            aligned = self.image_projection(image_features)
        
        return F.normalize(aligned, dim=-1)
    
    def encode_texts(self, texts):
        """ç¼–ç æ–‡æœ¬ï¼ˆä½¿ç”¨CLIPçš„æ–‡æœ¬ç¼–ç å™¨ï¼‰"""
        if isinstance(texts, str):
            texts = [texts]
        
        if len(texts) > 1000:
            batch_size = 1000
            all_embeddings = []
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_embeddings = self._encode_text_batch(batch_texts)
                all_embeddings.append(batch_embeddings)
            return torch.cat(all_embeddings, dim=0)
        else:
            return self._encode_text_batch(texts)
    
    def _encode_text_batch(self, texts):
        """ç¼–ç ä¸€æ‰¹æ–‡æœ¬"""
        inputs = self.clip_tokenizer(
            texts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=77
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            text_outputs = self.clip_model.get_text_features(**inputs)
        
        return F.normalize(text_outputs, dim=-1)
    
    def compute_similarity(self, image_features, text_features):
        """ğŸ†• è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦"""
        image_features = F.normalize(image_features, dim=-1)
        text_features = F.normalize(text_features, dim=-1)
        return torch.matmul(image_features, text_features.T)
    
    def forward(self, image_features=None, texts=None, raw_images=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            image_features: é¢„æå–çš„å›¾åƒç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            texts: å®ä½“æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
            raw_images: åŸå§‹å›¾åƒï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨ViTç¼–ç ï¼‰
        """
        results = {}
        
        # ä¼˜å…ˆä½¿ç”¨åŸå§‹å›¾åƒï¼ˆçœŸæ­£çš„ViTç¼–ç ï¼‰
        if raw_images is not None:
            vit_features = self.encode_images_from_raw(raw_images)
            if vit_features is not None:
                results['image_embeds'] = vit_features
                results['used_vit'] = True
        
        # å¦‚æœæ²¡æœ‰åŸå§‹å›¾åƒï¼Œä½¿ç”¨é¢„æå–ç‰¹å¾
        if 'image_embeds' not in results and image_features is not None:
            results['image_embeds'] = self.encode_images(image_features)
            results['used_vit'] = False
        
        if texts is not None:
            results['text_embeds'] = self.encode_texts(texts)
        
        return results


class ModalityReliabilityEstimator(nn.Module):
    """
    æ¨¡æ€å¯é æ€§è¯„ä¼°å™¨
    ä¸ºæ¯ä¸ªå®ä½“çš„æ¯ä¸ªæ¨¡æ€ä¼°è®¡ä¸€ä¸ªå¯é æ€§åˆ†æ•°
    """
    
    def __init__(self, modal_dims, hidden_dim=64):
        super(ModalityReliabilityEstimator, self).__init__()
        self.modal_dims = modal_dims
        
        # ä¸ºæ¯ä¸ªæ¨¡æ€åˆ›å»ºå¯é æ€§è¯„ä¼°ç½‘ç»œ
        self.reliability_nets = nn.ModuleDict()
        for modal_name, modal_dim in modal_dims.items():
            self.reliability_nets[modal_name] = nn.Sequential(
                nn.Linear(modal_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()
            )
        
        # ä½¿ç”¨æ™®é€šå±æ€§å­˜å‚¨ç»Ÿè®¡é‡ï¼ˆä¸èƒ½ç”¨register_bufferå­˜å‚¨å­—å…¸ï¼‰
        self.modal_means = {}
        self.modal_stds = {}
    
    def compute_feature_quality(self, features, modal_name):
        """
        è®¡ç®—ç‰¹å¾è´¨é‡åˆ†æ•°
        åŸºäºï¼š
        1. ç‰¹å¾æ–¹å·®ï¼ˆéšæœºç‰¹å¾é€šå¸¸æ–¹å·®è¾ƒå°ï¼‰
        2. ç‰¹å¾èŒƒæ•°ï¼ˆæ­£å¸¸ç‰¹å¾æœ‰åˆç†çš„èŒƒæ•°ï¼‰
        3. ç½‘ç»œé¢„æµ‹çš„å¯é æ€§
        """
        if features is None:
            return torch.zeros(1, device=next(self.parameters()).device)
        
        # 1. è®¡ç®—ç‰¹å¾ç»Ÿè®¡é‡
        feature_var = features.var(dim=-1, keepdim=True)  # [N, 1]
        feature_norm = features.norm(dim=-1, keepdim=True)  # [N, 1]
        
        # 2. å½’ä¸€åŒ–ç‰¹å¾
        features_normalized = F.normalize(features, dim=-1)
        
        # 3. ç½‘ç»œé¢„æµ‹
        if modal_name in self.reliability_nets:
            reliability = self.reliability_nets[modal_name](features)  # [N, 1]
        else:
            reliability = torch.ones(features.size(0), 1, device=features.device)
        
        # 4. ç»„åˆå¤šä¸ªä¿¡å·
        # ä½æ–¹å·®çš„ç‰¹å¾å¯èƒ½æ˜¯éšæœºå¡«å……çš„
        var_score = torch.sigmoid(feature_var * 10 - 0.5)  # æ–¹å·®å¤ªå°åˆ™åˆ†æ•°ä½
        norm_score = torch.sigmoid(feature_norm - 0.5)  # èŒƒæ•°å¤ªå°åˆ™åˆ†æ•°ä½
        
        quality_score = (reliability + var_score + norm_score) / 3.0
        
        return quality_score.squeeze(-1)
    
    def forward(self, modal_features):
        """
        è®¡ç®—æ‰€æœ‰æ¨¡æ€çš„å¯é æ€§åˆ†æ•°
        
        Args:
            modal_features: dict, {modal_name: features [N, D]}
        
        Returns:
            reliability_scores: dict, {modal_name: scores [N]}
        """
        reliability_scores = {}
        
        for modal_name, features in modal_features.items():
            if features is not None:
                reliability_scores[modal_name] = self.compute_feature_quality(
                    features, modal_name
                )
            else:
                reliability_scores[modal_name] = None
        
        return reliability_scores


class AdaptiveMultiModalFusion(nn.Module):
    """
    è‡ªé€‚åº”å¤šæ¨¡æ€èåˆå±‚
    æ”¹è¿›ç‚¹ï¼š
    1. å®ä½“çº§åˆ«çš„åŠ¨æ€æƒé‡ï¼ˆä¸æ˜¯å…¨å±€æƒé‡ï¼‰
    2. åŸºäºå¯é æ€§çš„åŠ æƒ
    3. è·¨æ¨¡æ€æ³¨æ„åŠ›äº¤äº’
    """
    
    def __init__(self, modal_dims, output_dim, use_cross_modal_attention=True):
        super(AdaptiveMultiModalFusion, self).__init__()
        self.modal_dims = modal_dims
        self.output_dim = output_dim
        self.use_cross_modal_attention = use_cross_modal_attention
        
        # æ ‡å‡†åŒ–ç»´åº¦
        self.standard_dim = 128
        
        # æ¨¡æ€æŠ•å½±å±‚
        self.modal_projections = nn.ModuleDict()
        for modal_name, modal_dim in modal_dims.items():
            self.modal_projections[modal_name] = nn.Sequential(
                nn.Linear(modal_dim, self.standard_dim),
                nn.LayerNorm(self.standard_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        
        # å¯é æ€§è¯„ä¼°å™¨
        self.reliability_estimator = ModalityReliabilityEstimator(modal_dims)
        
        # è‡ªé€‚åº”é—¨æ§ç½‘ç»œï¼ˆç”Ÿæˆå®ä½“çº§åˆ«çš„æ¨¡æ€æƒé‡ï¼‰
        num_modals = len(modal_dims)
        self.gate_network = nn.Sequential(
            nn.Linear(num_modals * self.standard_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_modals),
            nn.Softmax(dim=-1)
        )
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆå¯é€‰ï¼‰
        if use_cross_modal_attention:
            self.cross_attention = nn.MultiheadAttention(
                embed_dim=self.standard_dim,
                num_heads=4,
                dropout=0.1,
                batch_first=True
            )
        
        # æœ€ç»ˆæŠ•å½±
        self.output_projection = nn.Sequential(
            nn.Linear(self.standard_dim * num_modals, output_dim),
            nn.LayerNorm(output_dim)
        )
        
        # æ®‹å·®è¿æ¥çš„æŠ•å½±
        self.residual_projections = nn.ModuleDict()
        for modal_name in modal_dims.keys():
            self.residual_projections[modal_name] = nn.Linear(self.standard_dim, output_dim)
    
    def forward(self, modal_features, knowledge_context=None):
        """
        è‡ªé€‚åº”èåˆå¤šæ¨¡æ€ç‰¹å¾
        
        Args:
            modal_features: dict, {modal_name: features [N, D]}
            knowledge_context: å›¾ç»“æ„ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            fused_features: [N, output_dim]
        """
        device = next(self.parameters()).device
        
        # 1. æŠ•å½±æ‰€æœ‰æ¨¡æ€åˆ°æ ‡å‡†ç»´åº¦
        projected_features = {}
        valid_modals = []
        
        for modal_name in self.modal_dims.keys():
            if modal_name in modal_features and modal_features[modal_name] is not None:
                feat = modal_features[modal_name].to(device)
                projected = self.modal_projections[modal_name](feat)
                projected_features[modal_name] = projected
                valid_modals.append(modal_name)
        
        if not valid_modals:
            # æ²¡æœ‰æœ‰æ•ˆæ¨¡æ€ï¼Œè¿”å›é›¶å‘é‡
            batch_size = 1
            if knowledge_context is not None:
                batch_size = knowledge_context.size(0)
            return torch.zeros(batch_size, self.output_dim, device=device)
        
        # ç¡®ä¿batch sizeä¸€è‡´
        batch_size = min(feat.size(0) for feat in projected_features.values())
        for modal_name in projected_features:
            projected_features[modal_name] = projected_features[modal_name][:batch_size]
        
        # 2. è®¡ç®—æ¨¡æ€å¯é æ€§åˆ†æ•°
        reliability_scores = self.reliability_estimator(modal_features)
        
        # 3. å¦‚æœä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼Œè¿›è¡Œç‰¹å¾äº¤äº’
        if self.use_cross_modal_attention and len(valid_modals) > 1:
            # å°†æ‰€æœ‰æ¨¡æ€ç‰¹å¾å †å ä¸ºåºåˆ— [N, num_modals, D]
            modal_stack = torch.stack([projected_features[m] for m in valid_modals], dim=1)
            
            # è·¨æ¨¡æ€æ³¨æ„åŠ›
            attended, _ = self.cross_attention(modal_stack, modal_stack, modal_stack)
            
            # æ›´æ–°æŠ•å½±ç‰¹å¾
            for i, modal_name in enumerate(valid_modals):
                projected_features[modal_name] = attended[:, i, :]
        
        # 4. è®¡ç®—è‡ªé€‚åº”é—¨æ§æƒé‡
        # æ‹¼æ¥æ‰€æœ‰æŠ•å½±ç‰¹å¾
        concat_features = []
        for modal_name in self.modal_dims.keys():
            if modal_name in projected_features:
                concat_features.append(projected_features[modal_name])
            else:
                # å¯¹äºç¼ºå¤±æ¨¡æ€ï¼Œä½¿ç”¨é›¶å‘é‡
                concat_features.append(torch.zeros(batch_size, self.standard_dim, device=device))
        
        concat_features = torch.cat(concat_features, dim=-1)  # [N, num_modals * D]
        gate_weights = self.gate_network(concat_features)  # [N, num_modals]
        
        # 5. ç»“åˆå¯é æ€§åˆ†æ•°è°ƒæ•´æƒé‡
        adjusted_weights = []
        for i, modal_name in enumerate(self.modal_dims.keys()):
            if modal_name in reliability_scores and reliability_scores[modal_name] is not None:
                rel_score = reliability_scores[modal_name][:batch_size]
                adjusted_weight = gate_weights[:, i] * rel_score
            else:
                adjusted_weight = gate_weights[:, i] * 0.1  # ç¼ºå¤±æ¨¡æ€ç»™äºˆå¾ˆå°æƒé‡
            adjusted_weights.append(adjusted_weight)
        
        adjusted_weights = torch.stack(adjusted_weights, dim=-1)  # [N, num_modals]
        adjusted_weights = F.softmax(adjusted_weights, dim=-1)  # é‡æ–°å½’ä¸€åŒ–
        
        # 6. åŠ æƒèåˆ
        weighted_features = []
        for i, modal_name in enumerate(self.modal_dims.keys()):
            if modal_name in projected_features:
                weighted = projected_features[modal_name] * adjusted_weights[:, i:i+1]
            else:
                weighted = torch.zeros(batch_size, self.standard_dim, device=device)
            weighted_features.append(weighted)
        
        # æ‹¼æ¥åŠ æƒç‰¹å¾
        fused = torch.cat(weighted_features, dim=-1)  # [N, num_modals * D]
        
        # 7. æœ€ç»ˆæŠ•å½±
        output = self.output_projection(fused)
        
        # 8. æ®‹å·®è¿æ¥ï¼ˆä»æœ€å¯é çš„æ¨¡æ€ï¼‰
        if knowledge_context is not None and 'graph' in projected_features:
            residual = self.residual_projections['graph'](projected_features['graph'])
            output = output + 0.1 * residual
        
        return output
    
    def get_modal_weights(self, modal_features):
        """è·å–å½“å‰çš„æ¨¡æ€æƒé‡ï¼ˆç”¨äºåˆ†æï¼‰"""
        device = next(self.parameters()).device
        
        projected_features = {}
        for modal_name in self.modal_dims.keys():
            if modal_name in modal_features and modal_features[modal_name] is not None:
                feat = modal_features[modal_name].to(device)
                projected = self.modal_projections[modal_name](feat)
                projected_features[modal_name] = projected
        
        batch_size = min(feat.size(0) for feat in projected_features.values())
        
        concat_features = []
        for modal_name in self.modal_dims.keys():
            if modal_name in projected_features:
                concat_features.append(projected_features[modal_name][:batch_size])
            else:
                concat_features.append(torch.zeros(batch_size, self.standard_dim, device=device))
        
        concat_features = torch.cat(concat_features, dim=-1)
        gate_weights = self.gate_network(concat_features)
        
        return gate_weights


class CrossModalAlignmentModule(nn.Module):
    """
    è·¨æ¨¡æ€å¯¹é½æ¨¡å—
    å¼ºåˆ¶ä¸åŒæ¨¡æ€è¡¨ç¤ºåŒä¸€å®ä½“æ—¶çš„ä¸€è‡´æ€§
    """
    
    def __init__(self, modal_dims, align_dim=128):
        super(CrossModalAlignmentModule, self).__init__()
        self.modal_dims = modal_dims
        self.align_dim = align_dim
        
        # ä¸ºæ¯ä¸ªæ¨¡æ€åˆ›å»ºå¯¹é½æŠ•å½±
        self.align_projections = nn.ModuleDict()
        for modal_name, modal_dim in modal_dims.items():
            self.align_projections[modal_name] = nn.Sequential(
                nn.Linear(modal_dim, align_dim),
                nn.LayerNorm(align_dim)
            )
    
    def compute_alignment_loss(self, modal_features, train_links):
        """
        è®¡ç®—è·¨æ¨¡æ€å¯¹é½æŸå¤±
        
        å¯¹äºå¯¹é½çš„å®ä½“å¯¹(e1, e2)ï¼Œå®ƒä»¬åœ¨ä¸åŒæ¨¡æ€ä¸­çš„è¡¨ç¤ºåº”è¯¥ç›¸ä¼¼
        """
        device = next(self.parameters()).device
        
        # æ”¶é›†æœ‰æ•ˆæ¨¡æ€
        valid_modals = {k: v for k, v in modal_features.items() if v is not None}
        
        if len(valid_modals) < 2:
            return torch.tensor(0.0, device=device)
        
        total_loss = 0.0
        pair_count = 0
        
        modal_names = list(valid_modals.keys())
        
        # è®¡ç®—æ¯å¯¹æ¨¡æ€ä¹‹é—´çš„å¯¹é½æŸå¤±
        for i in range(len(modal_names)):
            for j in range(i + 1, len(modal_names)):
                modal_i = modal_names[i]
                modal_j = modal_names[j]
                
                feat_i = valid_modals[modal_i]
                feat_j = valid_modals[modal_j]
                
                # æŠ•å½±åˆ°å¯¹é½ç©ºé—´
                proj_i = self.align_projections[modal_i](feat_i)
                proj_j = self.align_projections[modal_j](feat_j)
                
                # å½’ä¸€åŒ–
                proj_i = F.normalize(proj_i, dim=-1)
                proj_j = F.normalize(proj_j, dim=-1)
                
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                min_size = min(proj_i.size(0), proj_j.size(0))
                valid_links = train_links[train_links[:, 0] < min_size]
                valid_links = valid_links[valid_links[:, 1] < min_size]
                
                if len(valid_links) == 0:
                    continue
                
                # å¯¹é½å®ä½“åº”è¯¥åœ¨ä¸åŒæ¨¡æ€ä¸­ç›¸ä¼¼
                aligned_i = proj_i[valid_links[:, 0]]
                aligned_j = proj_j[valid_links[:, 1]]
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±ï¼ˆé¼“åŠ±ç›¸ä¼¼ï¼‰
                similarity = F.cosine_similarity(aligned_i, aligned_j, dim=-1)
                alignment_loss = 1.0 - similarity.mean()
                
                total_loss += alignment_loss
                pair_count += 1
        
        if pair_count > 0:
            return total_loss / pair_count
        else:
            return torch.tensor(0.0, device=device)


class IBMultiModal(nn.Module):
    """
    æ”¹è¿›çš„å¤šæ¨¡æ€å®ä½“å¯¹é½æ¨¡å‹
    ä¸»è¦æ”¹è¿›ï¼š
    1. è‡ªé€‚åº”å¤šæ¨¡æ€èåˆ
    2. æ¨¡æ€å¯é æ€§è¯„ä¼°
    3. è·¨æ¨¡æ€å¯¹é½
    """
    
    def __init__(
        self,
        args,
        ent_num,
        img_feature_dim,
        char_feature_dim=None,
        use_project_head=False,
    ):
        super(IBMultiModal, self).__init__()

        self.args = args
        attr_dim = self.args.attr_dim
        img_dim = self.args.img_dim
        char_dim = self.args.char_dim
        dropout = self.args.dropout
        self.ENT_NUM = ent_num
        self.use_project_head = use_project_head

        # è§£æéšè—å±‚é…ç½®
        self.n_units = [int(x) for x in self.args.hidden_units.strip().split(",")]
        self.n_heads = [int(x) for x in self.args.heads.strip().split(",")]
        self.input_dim = self.n_units[0]
        
        # å®ä½“åµŒå…¥
        self.entity_emb = nn.Embedding(self.ENT_NUM, self.input_dim)
        nn.init.normal_(self.entity_emb.weight, std=1.0 / math.sqrt(self.ENT_NUM))
        self.entity_emb.requires_grad = True

        # ç‰¹å¾å¤„ç†å±‚ - æ”¹è¿›çš„æŠ•å½±
        self.rel_fc = nn.Sequential(
            nn.Linear(1000, attr_dim * 2),
            nn.LayerNorm(attr_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(attr_dim * 2, attr_dim)
        )
        
        self.att_fc = nn.Sequential(
            nn.Linear(1000, attr_dim * 2),
            nn.LayerNorm(attr_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(attr_dim * 2, attr_dim)
        )
        
        self.img_fc = nn.Sequential(
            nn.Linear(img_feature_dim, img_dim * 2),
            nn.LayerNorm(img_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(img_dim * 2, img_dim)
        )

        if char_feature_dim is None:
            char_feature_dim = 100
        self.name_fc = nn.Linear(300, char_dim)
        self.char_fc = nn.Linear(char_feature_dim, char_dim)

        # CLIPç›¸å…³ç»„ä»¶
        self.use_clip = getattr(args, 'use_clip', False)
        self.clip_encoder = None
        self.clip_dim = 0
        
        if self.use_clip and CLIP_AVAILABLE:
            try:
                self.clip_encoder = CLIPEntityEncoder(
                    clip_model_name=getattr(args, 'clip_model_name', 'openai/clip-vit-base-patch32'),
                    freeze_clip=getattr(args, 'clip_freeze', True),
                    device=getattr(args, 'device', None)
                )
                self.clip_dim = self.clip_encoder.clip_dim
                self.clip_projection = nn.Linear(self.clip_dim, img_dim)
                print(f"CLIP encoder initialized with dimension {self.clip_dim}")
            except Exception as e:
                print(f"Warning: Failed to initialize CLIP encoder: {e}")
                self.use_clip = False
                self.clip_encoder = None
                self.clip_dim = 0

        # å˜åˆ†ä¿¡æ¯ç“¶é¢ˆç›¸å…³
        self.use_graph_vib = getattr(args, 'use_graph_vib', False)
        self.use_attr_vib = getattr(args, 'use_attr_vib', False)
        self.use_img_vib = getattr(args, 'use_img_vib', False)
        self.use_rel_vib = getattr(args, 'use_rel_vib', False)
        self.use_joint_vib = getattr(args, 'use_joint_vib', False)

        # å˜åˆ†ç¼–ç å™¨
        if self.use_img_vib:
            self.img_fc_mu = nn.Linear(img_dim, img_dim)
            self.img_fc_std = nn.Linear(img_dim, img_dim)
        if self.use_rel_vib:
            self.rel_fc_mu = nn.Linear(attr_dim, attr_dim)
            self.rel_fc_std = nn.Linear(attr_dim, attr_dim)
        if self.use_attr_vib:
            self.att_fc_mu = nn.Linear(attr_dim, attr_dim)
            self.att_fc_std = nn.Linear(attr_dim, attr_dim)

        # å›¾ç»“æ„ç¼–ç å™¨
        no_diag = getattr(args, 'no_diag', False)
        diag = not no_diag
        self._init_structure_encoder(dropout, diag)

        # å¤šæ¨¡æ€èåˆ - ä½¿ç”¨æ”¹è¿›çš„è‡ªé€‚åº”èåˆ
        modal_dims = {}
        if args.w_gcn: modal_dims['graph'] = self.n_units[-1]
        if args.w_img: modal_dims['image'] = img_dim
        if args.w_rel: modal_dims['relation'] = attr_dim  
        if args.w_attr: modal_dims['attribute'] = attr_dim
        if args.w_name: modal_dims['name'] = char_dim
        if args.w_char: modal_dims['char'] = char_dim
        if self.use_clip: modal_dims['clip'] = img_dim
        
        # ä½¿ç”¨æ”¹è¿›çš„è‡ªé€‚åº”èåˆå±‚
        joint_dim = getattr(args, 'joint_dim', 600)
        self.fusion = AdaptiveMultiModalFusion(
            modal_dims=modal_dims,
            output_dim=joint_dim,
            use_cross_modal_attention=getattr(args, 'use_cross_modal_attention', True)
        )
        
        # è·¨æ¨¡æ€å¯¹é½æ¨¡å—
        self.cross_modal_alignment = CrossModalAlignmentModule(
            modal_dims=modal_dims,
            align_dim=128
        )

        # æŠ•å½±å¤´
        if self.use_project_head:
            self.img_pro = ProjectionHead(img_dim, img_dim, img_dim, dropout)
            self.att_pro = ProjectionHead(attr_dim, attr_dim, attr_dim, dropout)
            self.rel_pro = ProjectionHead(attr_dim, attr_dim, attr_dim, dropout)
            self.gph_pro = ProjectionHead(self.n_units[-1], self.n_units[-1], self.n_units[-1], dropout)

        # è”åˆå˜åˆ†ç¼–ç 
        if self.use_joint_vib:
            self.joint_fc = nn.Linear(joint_dim, joint_dim)
            self.joint_fc_mu = nn.Linear(joint_dim, joint_dim)
            self.joint_fc_std = nn.Linear(joint_dim, joint_dim)

        # KLDæŸå¤±è®°å½•
        self.kld_loss = 0.0
        self.img_kld_loss = 0.0
        self.rel_kld_loss = 0.0
        self.attr_kld_loss = 0.0
        self.joint_kld_loss = 0.0
        
        # CLIPç‰¹å¾è®°å½•
        self.clip_features = {}
        
        # æ¨¡æ€ç‰¹å¾è®°å½•ï¼ˆç”¨äºè·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼‰
        self.modal_features = {}

    def _init_structure_encoder(self, dropout, diag):
        """åˆå§‹åŒ–å›¾ç»“æ„ç¼–ç å™¨"""
        structure_encoder = getattr(self.args, 'structure_encoder', 'gcn')
        
        if structure_encoder == 'gcn':
            if self.use_graph_vib:
                # ğŸ†• Graph VIB: åˆ›å»ºmuå’Œlogvarä¸¤ä¸ªç½‘ç»œ
                self.cross_graph_model_mu = SimpleGCN(
                    self.input_dim, 
                    self.n_units[1] if len(self.n_units) > 1 else self.input_dim,
                    self.n_units[-1], 
                    dropout
                )
                self.cross_graph_model_logvar = SimpleGCN(
                    self.input_dim, 
                    self.n_units[1] if len(self.n_units) > 1 else self.input_dim,
                    self.n_units[-1], 
                    dropout
                )
            else:
                self.cross_graph_model = SimpleGCN(
                    self.input_dim, 
                    self.n_units[1] if len(self.n_units) > 1 else self.input_dim,
                    self.n_units[-1], 
                    dropout
                )
        elif structure_encoder == 'gat':
            if self.use_graph_vib:
                # ğŸ†• GAT + VIB
                self.cross_graph_model_mu = SimpleGAT(
                    self.n_units,
                    self.n_heads,
                    dropout,
                    getattr(self.args, 'attn_dropout', 0.0),
                    getattr(self.args, 'instance_normalization', False),
                    diag
                )
                self.cross_graph_model_logvar = SimpleGAT(
                    self.n_units,
                    self.n_heads,
                    dropout,
                    getattr(self.args, 'attn_dropout', 0.0),
                    getattr(self.args, 'instance_normalization', False),
                    diag
                )
            else:
                self.cross_graph_model = SimpleGAT(
                    self.n_units,
                    self.n_heads,
                    dropout,
                    getattr(self.args, 'attn_dropout', 0.0),
                    getattr(self.args, 'instance_normalization', False),
                    diag
                )
        else:
            self.cross_graph_model = SimpleGCN(
                self.input_dim,
                self.n_units[1] if len(self.n_units) > 1 else self.input_dim,
                self.n_units[-1],
                dropout
            )

    def _kld_gauss(self, mu_q, std_q, mu_p, std_p):
        """è®¡ç®—ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦"""
        kld = (
            torch.log(std_p / (std_q + 1e-8) + 1e-8)
            + (std_q ** 2 + (mu_q - mu_p) ** 2) / (2 * std_p ** 2 + 1e-8)
            - 0.5
        )
        return kld.mean()

    def forward(
        self,
        input_idx,
        adj,
        img_features=None,
        rel_features=None,
        att_features=None,
        name_features=None,
        char_features=None,
        entity_texts=None,
    ):
        """
        å‰å‘ä¼ æ’­
        """
        modal_features = {}
        
        # === å›¾ç»“æ„ç‰¹å¾å¤„ç† ===
        gph_emb = None
        if self.args.w_gcn:
            try:
                entity_input = self.entity_emb(input_idx)
                
                if self.use_graph_vib and hasattr(self, 'cross_graph_model_mu'):
                    # ğŸ†• Graph VIB: ä½¿ç”¨æ­£ç¡®çš„reparameterization
                    gph_mu = self.cross_graph_model_mu(entity_input, adj)
                    gph_logvar = self.cross_graph_model_logvar(entity_input, adj)
                    # çº¦æŸlogvarèŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
                    gph_logvar = torch.clamp(gph_logvar, -10, 2)
                    gph_std = torch.exp(0.5 * gph_logvar)
                    eps = torch.randn_like(gph_std)
                    gph_emb = gph_mu + eps * gph_std
                    # è®¡ç®—KLDæŸå¤±
                    self.kld_loss = self._kld_gauss(
                        gph_mu, gph_std, 
                        torch.zeros_like(gph_mu), torch.ones_like(gph_std)
                    )
                else:
                    gph_emb = self.cross_graph_model(entity_input, adj)
                    
                modal_features['graph'] = gph_emb
            except Exception as e:
                print(f"Warning: Graph encoding failed: {e}")
                entity_input = self.entity_emb(input_idx)
                gph_emb = entity_input
                modal_features['graph'] = gph_emb

        # === å›¾åƒç‰¹å¾å¤„ç† ===
        img_emb = None
        if self.args.w_img and img_features is not None:
            try:
                img_emb = self.img_fc(img_features)
                
                if self.use_img_vib and hasattr(self, 'img_fc_mu'):
                    img_emb_h = F.relu(img_emb)
                    mu = self.img_fc_mu(img_emb_h)
                    logvar = self.img_fc_std(img_emb_h)
                    std = torch.exp(0.5 * logvar)
                    eps = torch.randn_like(std)
                    img_emb = mu + eps * std
                    self.img_kld_loss = self._kld_gauss(
                        mu, std, torch.zeros_like(mu), torch.ones_like(std)
                    )
                
                modal_features['image'] = img_emb
            except Exception as e:
                print(f"Warning: Image encoding failed: {e}")

        # === å…³ç³»ç‰¹å¾å¤„ç† ===
        rel_emb = None
        if self.args.w_rel and rel_features is not None:
            try:
                rel_emb = self.rel_fc(rel_features)
                
                if self.use_rel_vib and hasattr(self, 'rel_fc_mu'):
                    rel_emb_h = F.relu(rel_emb)
                    mu = self.rel_fc_mu(rel_emb_h)
                    logvar = self.rel_fc_std(rel_emb_h)
                    std = torch.exp(0.5 * logvar)
                    eps = torch.randn_like(std)
                    rel_emb = mu + eps * std
                    self.rel_kld_loss = self._kld_gauss(
                        mu, std, torch.zeros_like(mu), torch.ones_like(std)
                    )
                
                modal_features['relation'] = rel_emb
            except Exception as e:
                print(f"Warning: Relation encoding failed: {e}")

        # === å±æ€§ç‰¹å¾å¤„ç† ===
        att_emb = None
        if self.args.w_attr and att_features is not None:
            try:
                att_emb = self.att_fc(att_features)
                
                if self.use_attr_vib and hasattr(self, 'att_fc_mu'):
                    att_emb_h = F.relu(att_emb)
                    mu = self.att_fc_mu(att_emb_h)
                    logvar = self.att_fc_std(att_emb_h)
                    std = torch.exp(0.5 * logvar)
                    eps = torch.randn_like(std)
                    att_emb = mu + eps * std
                    self.attr_kld_loss = self._kld_gauss(
                        mu, std, torch.zeros_like(mu), torch.ones_like(std)
                    )
                
                modal_features['attribute'] = att_emb
            except Exception as e:
                print(f"Warning: Attribute encoding failed: {e}")

        # === åç§°å’Œå­—ç¬¦ç‰¹å¾å¤„ç† ===
        name_emb = None
        if self.args.w_name and name_features is not None:
            try:
                name_emb = self.name_fc(name_features)
                modal_features['name'] = name_emb
            except Exception as e:
                print(f"Warning: Name encoding failed: {e}")
            
        char_emb = None
        if self.args.w_char and char_features is not None:
            try:
                char_emb = self.char_fc(char_features)
                modal_features['char'] = char_emb
            except Exception as e:
                print(f"Warning: Char encoding failed: {e}")

        # === CLIPç‰¹å¾å¤„ç† ===
        if self.use_clip and self.clip_encoder is not None:
            try:
                clip_results = {}
                
                if img_features is not None:
                    clip_results['image_embeds'] = self.clip_encoder.encode_images(img_features)
                    self.clip_features['image_embeds'] = clip_results['image_embeds']
                
                if entity_texts is not None:
                    clip_results['text_embeds'] = self.clip_encoder.encode_texts(entity_texts)
                    self.clip_features['text_embeds'] = clip_results['text_embeds']
                
                if 'image_embeds' in clip_results and 'text_embeds' in clip_results:
                    clip_fused = (clip_results['image_embeds'] + clip_results['text_embeds']) / 2
                    clip_projected = self.clip_projection(clip_fused)
                    modal_features['clip'] = clip_projected
                elif 'image_embeds' in clip_results:
                    clip_projected = self.clip_projection(clip_results['image_embeds'])
                    modal_features['clip'] = clip_projected
                elif 'text_embeds' in clip_results:
                    clip_projected = self.clip_projection(clip_results['text_embeds'])
                    modal_features['clip'] = clip_projected
                    
            except Exception as e:
                print(f"Warning: CLIP encoding failed: {e}")

        # === æŠ•å½±å¤´å¤„ç† ===
        if self.use_project_head:
            if 'image' in modal_features and modal_features['image'] is not None:
                modal_features['image'] = self.img_pro(modal_features['image'])
            if 'attribute' in modal_features and modal_features['attribute'] is not None:
                modal_features['attribute'] = self.att_pro(modal_features['attribute'])
            if 'relation' in modal_features and modal_features['relation'] is not None:
                modal_features['relation'] = self.rel_pro(modal_features['relation'])
            if 'graph' in modal_features and modal_features['graph'] is not None:
                modal_features['graph'] = self.gph_pro(modal_features['graph'])

        # ä¿å­˜æ¨¡æ€ç‰¹å¾ï¼ˆç”¨äºè·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼‰
        self.modal_features = modal_features

        # === å¤šæ¨¡æ€èåˆ ===
        try:
            knowledge_context = modal_features.get('graph', None)
            joint_emb = self.fusion(modal_features, knowledge_context)
        except Exception as e:
            print(f"Warning: Fusion failed: {e}")
            joint_emb = self._fallback_fusion(modal_features, input_idx)

        # === è”åˆå˜åˆ†ç¼–ç  ===
        if self.use_joint_vib and hasattr(self, 'joint_fc_mu'):
            try:
                joint_emb = self.joint_fc(joint_emb)
                joint_emb_h = F.relu(joint_emb)
                mu = self.joint_fc_mu(joint_emb_h)
                logvar = self.joint_fc_std(joint_emb_h)
                std = torch.exp(0.5 * logvar)
                eps = torch.randn_like(std)
                joint_emb = mu + eps * std
                self.joint_kld_loss = self._kld_gauss(
                    mu, std, torch.zeros_like(mu), torch.ones_like(std)
                )
            except Exception as e:
                print(f"Warning: Joint VIB failed: {e}")

        return gph_emb, img_emb, rel_emb, att_emb, name_emb, char_emb, joint_emb

    def _fallback_fusion(self, modal_features, input_idx):
        """å¤‡ç”¨èåˆæ–¹æ³•"""
        valid_embeddings = [emb for emb in modal_features.values() if emb is not None]
        
        if valid_embeddings:
            standard_dim = 128
            projected_embeddings = []
            
            for i, emb in enumerate(valid_embeddings):
                proj_attr_name = f'fallback_projection_{i}'
                if not hasattr(self, proj_attr_name):
                    setattr(self, proj_attr_name, 
                           nn.Linear(emb.size(-1), standard_dim).to(emb.device))
                proj_layer = getattr(self, proj_attr_name)
                projected_embeddings.append(proj_layer(emb))
            
            joint_emb = torch.cat(projected_embeddings, dim=-1)
            
            if not hasattr(self, 'final_fallback_projection'):
                input_dim = joint_emb.size(-1)
                output_dim = getattr(self.args, 'joint_dim', 600)
                self.final_fallback_projection = nn.Linear(input_dim, output_dim).to(joint_emb.device)
            joint_emb = self.final_fallback_projection(joint_emb)
        else:
            entity_input = self.entity_emb(input_idx)
            output_dim = getattr(self.args, 'joint_dim', 600)
            if not hasattr(self, 'emergency_projection'):
                self.emergency_projection = nn.Linear(entity_input.size(-1), output_dim).to(entity_input.device)
            joint_emb = self.emergency_projection(entity_input)
        
        return joint_emb
    
    def get_cross_modal_alignment_loss(self, train_links):
        """è·å–è·¨æ¨¡æ€å¯¹é½æŸå¤±"""
        if hasattr(self, 'modal_features') and self.modal_features:
            return self.cross_modal_alignment.compute_alignment_loss(
                self.modal_features, train_links
            )
        return torch.tensor(0.0)


# ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œè®¾ç½®åˆ«å
KAMultiModal = IBMultiModal
ClipEnhancedKACEA = IBMultiModal
KACEA = IBMultiModal